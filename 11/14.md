

context

kubeadm 配置的集群已迁移到新的机器 ，他需要更改配置才能够运行成功

Task

修复在机器迁移过程中损坏的单节点集群。

首先，确定损坏集群组件并调查导致损坏的原因
> 已经停用的集群使用外部的 etcd服务器

接下来 ，修复所有损坏的集群的组件的配置


最后确保 集群运行正常
 * 单个节点和所有pod 都处于Ready 状态



 我将帮您诊断和修复迁移后损坏的单节点 Kubernetes 集群。以下是详细的解决步骤：

## 步骤 1：诊断集群状态

首先，检查集群各组件的状态：

```bash
# 检查节点状态
kubectl get nodes

# 检查所有 Pod 状态
kubectl get pods --all-namespaces

# 检查 kube-system 命名空间中的核心组件
kubectl get pods -n kube-system

# 检查 etcd 状态
kubectl get pods -n kube-system | grep etcd
```

## 步骤 2：检查组件日志

```bash
# 检查 kubelet 状态和日志
sudo systemctl status kubelet
sudo journalctl -u kubelet -f --no-pager

# 检查容器运行时状态
sudo systemctl status docker
# 或者如果是 containerd:
sudo systemctl status containerd

# 检查 API Server 日志
kubectl logs -n kube-system kube-apiserver-$(hostname) --prefix=true
```

## 步骤 3：常见问题诊断

### 检查网络配置

```bash
# 检查网络接口和 IP 地址
ip addr show
hostname

# 检查 /etc/hosts 文件
cat /etc/hosts

# 检查网络连接
ping -c 3 $(hostname)
```

### 检查证书和配置

```bash
# 检查 kubelet 配置
sudo cat /var/lib/kubelet/kubeadm-flags.env

# 检查 kubeconfig 文件
sudo cat /etc/kubernetes/admin.conf

# 检查证书有效期
sudo kubeadm certs check-expiration
```

## 步骤 4：修复外部 etcd 连接

由于集群使用外部 etcd，需要检查 etcd 连接配置：

```bash
# 检查 etcd 静态 Pod 清单
sudo cat /etc/kubernetes/manifests/etcd.yaml

# 检查 API Server 的 etcd 配置
sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep etcd
```

如果 etcd 服务器地址发生变化，需要更新配置：

```bash
# 备份原有配置
sudo cp /etc/kubernetes/manifests/kube-apiserver.yaml /etc/kubernetes/manifests/kube-apiserver.yaml.backup

# 编辑 API Server 配置，更新 etcd 服务器地址
sudo vim /etc/kubernetes/manifests/kube-apiserver.yaml
```

在 `kube-apiserver.yaml` 中查找并更新 etcd 相关参数：
```yaml
- --etcd-servers=https://NEW_ETCD_IP:2379
- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
- --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
- --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
```

## 步骤 5：修复节点 IP 地址变化

如果节点 IP 地址发生变化：

```bash
# 检查当前节点信息
kubectl get nodes -o wide

# 如果节点 IP 不正确，需要更新 kubelet 配置
sudo vim /var/lib/kubelet/kubeadm-flags.env

# 在 KUBELET_KUBEADM_ARGS 中添加或修改节点 IP
# 例如：--node-ip=<NEW_NODE_IP>
```

## 步骤 6：重新生成证书（如果需要）

如果证书包含旧的主机名或 IP：

```bash
# 备份现有证书
sudo cp -r /etc/kubernetes/pki /etc/kubernetes/pki.backup

# 重新生成证书（谨慎操作，这会重置集群）
sudo kubeadm init phase certs all --apiserver-advertise-address <NEW_IP> --apiserver-cert-extra-sans <NEW_HOSTNAME>

# 或者使用更安全的方式，只更新特定证书
sudo kubeadm certs renew all
```

## 步骤 7：修复 kubeconfig 文件

```bash
# 更新管理员 kubeconfig
sudo kubeadm init phase kubeconfig admin --apiserver-advertise-address <NEW_IP>

# 复制新的 kubeconfig 到用户目录
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

## 步骤 8：重启集群组件

```bash
# 重启 kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 等待组件重新启动
sleep 30

# 检查组件状态
kubectl get pods -n kube-system
```

## 步骤 9：验证修复

```bash
# 检查节点状态（应该显示 Ready）
kubectl get nodes

# 检查所有核心组件
kubectl get pods -n kube-system

# 运行集群诊断
kubectl get componentstatuses

# 测试集群功能
kubectl create deployment test-nginx --image=nginx
kubectl get pods -l app=test-nginx

# 清理测试资源
kubectl delete deployment test-nginx
```

## 完整的修复脚本

这是一个综合修复脚本，可以根据实际情况调整：

```bash
#!/bin/bash

echo "=== Kubernetes 集群修复脚本 ==="

# 1. 停止 kubelet
sudo systemctl stop kubelet

# 2. 备份关键文件
sudo cp -r /etc/kubernetes /etc/kubernetes.backup
sudo cp -r /var/lib/kubelet /var/lib/kubelet.backup

# 3. 获取新的节点信息
NEW_IP=$(hostname -I | awk '{print $1}')
HOSTNAME=$(hostname)

echo "新 IP: $NEW_IP"
echo "主机名: $HOSTNAME"

# 4. 更新 kubelet 配置
sudo sed -i "s/--node-ip=[0-9.]*/--node-ip=$NEW_IP/" /var/lib/kubelet/kubeadm-flags.env

# 5. 更新 /etc/hosts（如果需要）
echo "$NEW_IP $HOSTNAME" | sudo tee -a /etc/hosts

# 6. 重新生成证书
sudo kubeadm certs renew all

# 7. 启动 kubelet
sudo systemctl daemon-reload
sudo systemctl start kubelet

# 8. 等待集群稳定
echo "等待集群组件启动..."
sleep 60

# 9. 验证修复
echo "=== 验证集群状态 ==="
kubectl get nodes
kubectl get pods -n kube-system

echo "修复完成！"
```

## 重要注意事项

1. **备份重要数据**：在执行任何修复操作前备份关键配置和证书
2. **外部 etcd**：确保外部 etcd 服务器可访问且数据完整
3. **网络配置**：验证网络接口、DNS 解析和防火墙规则
4. **证书有效期**：检查所有 Kubernetes 证书是否在有效期内
5. **逐步验证**：每个修复步骤后验证集群状态

完成这些步骤后，您的单节点 Kubernetes 集群应该恢复正常，所有节点和 Pod 都处于 Ready 状态。